{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úçÔ∏è From A to Z\n",
    "\n",
    "Our goal is to go from data extracted from the [ChemArXiV](https://chemrxiv.org/engage/chemrxiv/public-dashboard) dataset by Marta to a dataset of synthetic data that could be used to train models in the Chemistry domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n fromatoz --file ./package-list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start by taking the data and transforming it into a huggingface dataset. This is done by temporarily transforming it into a .json file before reading the json and transforming it into a [arrow](https://arrow.apache.org/) file format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries for this first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the input and temp. output dir. ‚ö†Ô∏è Change this to your directory !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DIR = \"datasets/chemrxiv_papers\"\n",
    "DEST_DIR = \"datasets/temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to take the opportunity of embedding the data into the different templates we want to use during the rephrasing part. We define them beneath, they were taken off the [cosmopedia](https://github.com/huggingface/cosmopedia/tree/main) github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLES = {\"wikihow\":\n",
    "\"\"\"Here is an extract from a webpage: \"<INSERT_EXTRACT>\".\n",
    "\n",
    "Write a long and very detailed tutorial that could be part of WikiHow whose title is related to the extract above <ADD_TOPIC>. Include in depth explanations for each step and how it helps achieve the desired outcome, inluding key tips and guidelines. \n",
    "Ensure clarity and practicality, allowing readers to easily follow and apply the instructions. Do not use images.\"\"\",\n",
    "\n",
    "\"textbook_narrative\":\n",
    "\"\"\"Here is an extract from a webpage: \"<INSERT_EXTRACT>\".\n",
    "\n",
    "Write an extensive and detailed course unit suitable for a textbook, related to the given extract <ADD_TOPIC>. Do not just list concepts, but develop each one in detail before moving to the next, as we prioritize depth of understanding and comprehensive exploration of the subject matter over breadth. Focus on:\n",
    "\n",
    "- Rigor: Ensure in-depth coverage of the concepts.\n",
    "- Engagement: Use a narrative style akin to Michael Lewis, making it captivating and thought-provoking.\n",
    "- Relevance: Connect the topic with current trends, real-life examples, or recent studies. Do not use images.\n",
    "Do not include a title or an introduction, simply write the content without headlines and introductory phrases. Do not use images.\"\"\",\n",
    "\n",
    "\"textbook_academic\":\n",
    "\"\"\"Here is an extract from a webpage: \"<INSERT_EXTRACT>\".\n",
    "\n",
    "Write an extensive and detailed course unit suitable for a textbook targeted at college students, related to the given extract <ADD_TOPIC>. Do not just list concepts, but develop each one in detail before moving to the next, as we prioritize depth of understanding and comprehensive exploration of the subject matter over breadth. Focus on:\n",
    "\n",
    "- Rigor: Ensure in-depth coverage of the concepts/sections.\n",
    "- Engagement: Write with an academic, professional and engaging tone that captivates interest.\n",
    "- Application: Incorporate specific, practical examples, such as proofs in calculus or critical dates and figures in history.\n",
    "Do not include a title or an introduction, simply write the content without headlines and introductory phrases. Do not use images.\"\"\",\n",
    "\n",
    "\"blogpost\":\n",
    "\"\"\"Here is an extract from a webpage: \"<INSERT_EXTRACT>\".\n",
    "\n",
    "Write an informative and insightful blog post that expands upon the extract above <ADD_TOPIC>. Your post should delve into the nuances of the topic, offering fresh perspectives and deeper analysis. Aim to:\n",
    "\n",
    "- Inform: Provide valuable, well-researched information that educates the reader.\n",
    "- Engage: Write in a conversational tone that connects with the audience, making complex ideas accessible.\n",
    "- Illustrate: Use examples, anecdotes, or personal experiences to bring the topic to life.\n",
    "Do not give a title and do not start with sentences like \"Have you ever...\" or \"Hello dear readers..\", simply write the content without these introductory phrases.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex part, we iterate over the files and attempt to only keep relevant paragraphs. We multithread to speed up the process and define min. and max. text lenghts we with to extract. Based off [Rephrasing the Web](https://arxiv.org/abs/2401.16380) we keep it underneath 400 words as more makes the model lose context during sythesizing.\n",
    "\n",
    "We also keep the title as it can be used for further context down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 343.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace() argument 2 must be str, not None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MIN_TEXT_LENGTH = 200\n",
    "MAX_TEXT_LENGTH = 400\n",
    "\n",
    "def save_extracted_data(file_path, data):\n",
    "    with open(file_path, 'a') as file:\n",
    "        for sample in data:\n",
    "            json.dump(sample, file)\n",
    "            file.write('\\n')  # Add newline character to separate JSON objects\n",
    "\n",
    "\n",
    "def extract_text(filepath):\n",
    "    source_paper_path = f\"{os.getcwd()}/{SOURCE_DIR}/{filepath}\"\n",
    "    sample = {\n",
    "        \"filename\": filepath,\n",
    "        \"title\": \"\",  # Initialize title\n",
    "        \"texts\": []   # Initialize list to store extracted text\n",
    "    }\n",
    "    tree = ET.parse(source_paper_path)\n",
    "    try:\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Define the namespace map\n",
    "        ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "        # Extract title\n",
    "        title = \"\"\n",
    "        teiHeader_elem = root.find('tei:teiHeader', namespaces=ns)\n",
    "        if teiHeader_elem is not None:\n",
    "            fileDesk_elem = teiHeader_elem.find('tei:fileDesc', namespaces=ns)\n",
    "            if fileDesk_elem is not None:\n",
    "                titleStmt_elem = fileDesk_elem.find('tei:titleStmt', namespaces=ns)\n",
    "                if titleStmt_elem is not None:\n",
    "                    title_elem = titleStmt_elem.find('tei:title', namespaces=ns)\n",
    "                    if title_elem is not None:\n",
    "                        title = title_elem.text\n",
    "                    else:\n",
    "                        print(\"No <title> element found.\")\n",
    "                else:\n",
    "                    print(\"No <titleStmt> element found.\")\n",
    "            else:\n",
    "                print(\"No <fileDesc> element found.\")\n",
    "        else:\n",
    "            print(\"No <teiHeader> element found.\")\n",
    "        sample['title'] = title\n",
    "\n",
    "        # Extract text\n",
    "        text_list = []\n",
    "        text_elem = root.find('tei:text', namespaces=ns)\n",
    "        if text_elem is not None:\n",
    "            body_elem = text_elem.find('tei:body', namespaces=ns)\n",
    "            if body_elem is not None:\n",
    "                texts = body_elem.findall('tei:div', namespaces=ns)\n",
    "                for div in texts:\n",
    "                    text = ' '.join(div.itertext())\n",
    "                    for prompt in STYLES.values():\n",
    "                        if len(text.split(\" \")) > MIN_TEXT_LENGTH and len(text.split(\" \")) < MAX_TEXT_LENGTH: \n",
    "                            text2 = prompt.replace(\"<ADD_TOPIC>\", title).replace(\"<INSERT_EXTRACT>\", text)\n",
    "                            text_list.append(text2)\n",
    "        sample['texts'] = text_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return sample\n",
    "\n",
    "def main():\n",
    "    filenames = [k for k in os.listdir(os.getcwd() + \"/\" + SOURCE_DIR + \"/\") if k.endswith('.xml')]\n",
    "    # print(os.getcwd() + \"/\" + DEST_DIR)\n",
    "    print(len(filenames))\n",
    "    batch_size = 100  # Adjust batch size as needed\n",
    "    total_batches = len(filenames) // batch_size + 1\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        for batch_num in range(total_batches):\n",
    "            batch_filenames = filenames[batch_num * batch_size: (batch_num + 1) * batch_size]\n",
    "            extracted_data = []\n",
    "            for sample in tqdm(executor.map(extract_text, batch_filenames), total=len(batch_filenames), desc=f\"Processing Batch {batch_num + 1}/{total_batches}\"):\n",
    "                extracted_data.append(sample)\n",
    "            save_extracted_data(f'{DEST_DIR}/data.json', extracted_data)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a json file with the collected data and transform it into the arrow format for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24449it [00:00, 377545.11it/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 440/440 [00:00<00:00, 206177.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_dict = {\n",
    "        \"title\": [],\n",
    "        \"filename\": [],\n",
    "        \"text\": []\n",
    "    }\n",
    "\n",
    "with jsonlines.open(f'{DEST_DIR}/data.json') as reader:\n",
    "    for paper in tqdm(reader):\n",
    "        for text in paper[\"texts\"]:\n",
    "            dataset_dict[\"title\"].append(paper[\"title\"])\n",
    "            dataset_dict[\"filename\"].append(paper[\"filename\"])\n",
    "            dataset_dict[\"text\"].append(text)\n",
    "\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    dataset.save_to_disk(\"datasets/huggingface_dataset_large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ú® Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset in the arrows format we can start synthesizing some data. To be as efficient as possible and allow for iterative testing, we use a modified version of llm_swarm that allows for the creation of side-by-side pods running [TGI](https://huggingface.co/docs/text-generation-inference/index) with whatever model that is available.\n",
    "\n",
    "‚ö†Ô∏è Replace YOURTOKENHERE by your huggingface token (cat ~/.cache/huggingface/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerjan/miniconda3/envs/fresh/lib/python3.10/site-packages/huggingface_hub-0.23.3-py3.8.egg/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': Value(dtype='string', id=None), 'filename': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}\n",
      "üëé Aborted! Waiting for runai-1715862733-0 to be created                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1' coro=<main() done, defined at /tmp/ipykernel_3598626/2030642281.py:56> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3598626/2030642281.py\", line 92, in <module>\n",
      "    asyncio.run(main())\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/tmp/ipykernel_3598626/2030642281.py\", line 59, in main\n",
      "    with LLMSwarm(isc) as llm_swarm:\n",
      "  File \"/home/kerjan/fresh/./llm_swarm/llm_swarm/__init__.py\", line 127, in __enter__\n",
      "    self.start()\n",
      "  File \"/home/kerjan/fresh/./llm_swarm/llm_swarm/__init__.py\", line 59, in start\n",
      "    self._wait_for_jobs_to_start(job_ids)\n",
      "  File \"/home/kerjan/fresh/./llm_swarm/llm_swarm/__init__.py\", line 78, in _wait_for_jobs_to_start\n",
      "    while not self.scheduler.is_job_running(job_id):\n",
      "  File \"/home/kerjan/fresh/./llm_swarm/llm_swarm/schedulers/runai_scheduler.py\", line 47, in is_job_running\n",
      "    result = run_command(command)\n",
      "  File \"/home/kerjan/fresh/./llm_swarm/llm_swarm/utils.py\", line 15, in run_command\n",
      "    output, errors = process.communicate()\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/subprocess.py\", line 1154, in communicate\n",
      "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/subprocess.py\", line 2021, in _communicate\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/kerjan/miniconda3/envs/fresh/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m     91\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m---> 92\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fresh/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/miniconda3/envs/fresh/lib/python3.10/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fresh/lib/python3.10/site-packages/nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/fresh/lib/python3.10/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fresh/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[15], line 59\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     57\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m LLMSwarm(isc) \u001b[38;5;28;01mas\u001b[39;00m llm_swarm:\n\u001b[1;32m     60\u001b[0m         semaphore \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mSemaphore(llm_swarm\u001b[38;5;241m.\u001b[39msuggested_max_parallel_requests)\n\u001b[1;32m     61\u001b[0m         client \u001b[38;5;241m=\u001b[39m AsyncInferenceClient(model\u001b[38;5;241m=\u001b[39mllm_swarm\u001b[38;5;241m.\u001b[39mendpoint)\n",
      "File \u001b[0;32m~/fresh/./llm_swarm/llm_swarm/__init__.py:127\u001b[0m, in \u001b[0;36mLLMSwarm.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/fresh/./llm_swarm/llm_swarm/__init__.py:59\u001b[0m, in \u001b[0;36mLLMSwarm.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m host_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m job_timestamp\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1715862733\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_jobs_to_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_endpoints_to_be_reachable(host_path, job_ids)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# print(f\"{self.config.job_scheduler} Job ID: {self.job_ids}\")\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# print(f\"üìñ {self.config.job_scheduler} hosts path: {host_path}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/fresh/./llm_swarm/llm_swarm/__init__.py:79\u001b[0m, in \u001b[0;36mLLMSwarm._wait_for_jobs_to_start\u001b[0;34m(self, job_ids)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Loader(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be created\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mis_job_running(job_id):\n\u001b[0;32m---> 79\u001b[0m         \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m log_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlogs_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm-swarm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìñ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mjob_scheduler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m log path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"./llm_swarm\")\n",
    "from llm_swarm import LLMSwarm, LLMSwarmConfig\n",
    "from huggingface_hub import AsyncInferenceClient\n",
    "from transformers import AutoTokenizer\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import time\n",
    "\n",
    "# Define your LLMSwarmConfig\n",
    "isc = LLMSwarmConfig(\n",
    "    instances=1, #Number of instances\n",
    "    inference_engine=\"tgi\", #The engine to use. (Could use vLLM)\n",
    "    job_scheduler=\"runai\", #The scheduler to use (would otherwise be slurm)\n",
    "    gpus=1, #Number of GPUs to use per instance\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\", #Model for inference\n",
    "    template_path=\"llm_swarm/templates/tgi.template.yml\", #Template used for the run\n",
    "    load_balancer_template_path=\"llm_swarm/templates/nginx.template.conf\", #Load distributor\n",
    "    huggingface_token=\"YOURTOKENHERE\", #Huggingface token\n",
    "    model_max_total=3000,\n",
    "    model_max_input=1200,\n",
    "    per_instance_max_parallel_requests=300,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(isc.model)\n",
    "\n",
    "# Load dataset\n",
    "ds = Dataset.load_from_disk(\"datasets/huggingface_dataset_large\")\n",
    "print(ds.features)\n",
    "ds = ds.select(range(0, 100))\n",
    "\n",
    "# Define your processing function\n",
    "async def process_text(task, client, semaphore, tokenizer):\n",
    "    async with semaphore:\n",
    "        prompt = rf\"<s>[INST] {task['text']} [\\INST]\"\n",
    "        completion = await client.text_generation(\n",
    "            prompt=prompt,\n",
    "            max_new_tokens=1000,\n",
    "            stop_sequences=[\"User:\", \"###\", \"<|endoftext|>\"],\n",
    "            repetition_penalty=1.3,\n",
    "        )\n",
    "        tokenized_completion = tokenizer.encode(completion)\n",
    "        token_length = len(tokenized_completion)\n",
    "        return task[\"title\"], task[\"filename\"], task[\"text\"], completion, token_length\n",
    "\n",
    "# Save results function\n",
    "def save_results(results):\n",
    "    with open(\"results.json\", \"a\") as file:\n",
    "        json.dump(results, file)\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "# Main processing function\n",
    "async def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    with LLMSwarm(isc) as llm_swarm:\n",
    "        semaphore = asyncio.Semaphore(llm_swarm.suggested_max_parallel_requests)\n",
    "        client = AsyncInferenceClient(model=llm_swarm.endpoint)\n",
    "\n",
    "        tasks = [\n",
    "            process_text(task, client, semaphore, tokenizer) for task in ds\n",
    "        ]\n",
    "        results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_duration = end_time - start_time\n",
    "        total_tokens = sum(result[4] for result in results)\n",
    "        overall_tokens_per_second = total_tokens / total_duration if total_duration > 0 else 0\n",
    "\n",
    "        # Prepare processed data\n",
    "        processed_data = {\n",
    "            \"title\": [result[0] for result in results],\n",
    "            \"filename\": [result[1] for result in results],\n",
    "            \"original_text\": [result[2] for result in results],\n",
    "            \"processed_text\": [result[3] for result in results],\n",
    "            \"token_length\": [result[4] for result in results],\n",
    "        }\n",
    "\n",
    "        processed_ds = Dataset.from_dict(processed_data)\n",
    "        processed_ds.save_to_disk(\"synthetic_data\")\n",
    "\n",
    "        # processed_ds.push_to_hub(\"TugdualKerjan/SynthChem\")\n",
    "\n",
    "        print(f\"Overall Tokens per Second: {overall_tokens_per_second}\")\n",
    "        save_results(results)\n",
    "\n",
    "# Run the main function\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
